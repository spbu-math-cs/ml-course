{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"./images/logo_fmkn.png\" width=300 style=\"display: inline-block;\"></center> \n",
    "\n",
    "## Машинное обучение 2\n",
    "### Семинар 3. Рекуррентная нейронная сеть\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "3 марта 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источник вдохновения: https://github.com/andriygav/MachineLearningSeminars/tree/master/sem15\n",
    "\n",
    "Видео семинара МФТИ https://www.youtube.com/watch?v=_zhKsIze8QU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s3qWY0M5LA6r"
   },
   "source": [
    "### Импортируем библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "K2_VhyWeteMB"
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import gridspec\n",
    "from PIL import Image\n",
    "import io\n",
    "from urllib.request import urlopen\n",
    "from lime import lime_image\n",
    "from skimage.segmentation import mark_boundaries\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "UCn8xDPhteMB"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h61Q8xrnqDUV"
   },
   "source": [
    "### Зададим устройство исполнения кода (cpu/cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkFMQL4RqDUV",
    "outputId": "1abaa6d8-02d0-4450-8e7e-0633d9b61394"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rh0dr4tqDUc"
   },
   "source": [
    "### Рекурентная нейронная сеть (seq2seq архитектура, encoder-decoder) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./images/enc_dec_linear_out-min.png\" width=600></center> \n",
    "\n",
    "-----\n",
    "Источник: https://lena-voita.github.io/nlp_course/seq2seq_and_attention.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia0PcGE__GBh"
   },
   "source": [
    "#### Полезный код для обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dbVvmM0mqDUe"
   },
   "outputs": [],
   "source": [
    "def batch_generator(dataset, char2idx, batch_size=64, shuffle=True):\n",
    "    X, Y = dataset[:-1], dataset[1:]\n",
    "    \n",
    "    PAD = char2idx['<PAD>']\n",
    "    n_samples = len(X)\n",
    "\n",
    "# генерим список индексов\n",
    "    list_of_indexes = np.linspace(\n",
    "        0, n_samples - 1, n_samples, dtype=np.int64)\n",
    "    List_X = []\n",
    "    List_Y = []\n",
    "    \n",
    "# если нужно перемешать, то перемешиваем\n",
    "    if shuffle:\n",
    "        np.random.shuffle(list_of_indexes)        \n",
    "\n",
    "# сгенерируем список индексов, по этим индексам,\n",
    "# сделаем новый перемешаный список токенов и тэгов\n",
    "    for indx in list_of_indexes:\n",
    "        List_X.append(X[indx])\n",
    "        List_Y.append(Y[indx])\n",
    "    \n",
    "    n_batches = n_samples // batch_size\n",
    "    if n_samples % batch_size != 0:\n",
    "        n_batches += 1\n",
    "        \n",
    "    # For each k yield pair x and y\n",
    "    for k in range(n_batches):\n",
    "# указываем текущии размер батча\n",
    "        this_batch_size = batch_size\n",
    "    \n",
    "# если мы выдаем последний батч, то его нужно обрезать\n",
    "        if k == n_batches - 1:\n",
    "            if n_samples % batch_size > 0:\n",
    "                this_batch_size = n_samples % batch_size\n",
    "                \n",
    "        This_X = List_X[k*batch_size:k*batch_size + this_batch_size]\n",
    "        This_Y = List_Y[k*batch_size:k*batch_size + this_batch_size]\n",
    "        \n",
    "        This_X_line = [\n",
    "                       [char2idx.get(char, 0) for char in sent]\\\n",
    "                       for sent in This_X]\n",
    "        This_Y_line = [\n",
    "                       [char2idx.get('<START>', 0)]\\\n",
    "                       + [char2idx.get(char, 0) for char in sent]\\\n",
    "                       + [char2idx.get('<FINISH>', 0)]\\\n",
    "                       for sent in This_Y]\n",
    "\n",
    "        List_of_length_x = [len(sent) for sent in This_X_line]\n",
    "        length_of_sentence_x = max(List_of_length_x)\n",
    "        List_of_length_y = [len(sent) for sent in This_Y_line]\n",
    "        length_of_sentence_y = max(List_of_length_y)\n",
    "\n",
    "        x_arr = np.ones(shape=[this_batch_size, length_of_sentence_x])*PAD\n",
    "        y_arr = np.ones(shape=[this_batch_size, length_of_sentence_y])*PAD\n",
    "\n",
    "        for i in range(this_batch_size):\n",
    "            x_arr[i, :len(This_X_line[i])] = This_X_line[i]\n",
    "            y_arr[i, :len(This_Y_line[i])] = This_Y_line[i]\n",
    "\n",
    "        x = torch.LongTensor(x_arr)\n",
    "        y = torch.LongTensor(y_arr)\n",
    "        lengths = torch.LongTensor(List_of_length_x)\n",
    "\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yJbyhRfzqDUe"
   },
   "outputs": [],
   "source": [
    "def train_on_batch(model, batch_of_x, batch_of_y, optimizer, loss_function):\n",
    "    encoder, decoder = model\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    encoder.zero_grad()\n",
    "    decoder.zero_grad()\n",
    "    \n",
    "    d, h, c = encoder(batch_of_x.to(encoder.device))\n",
    "    output = decoder(\n",
    "        batch_of_y.to(decoder.device), \n",
    "        h=h.to(decoder.device)[:, -decoder.num_layers:, :], \n",
    "        c=c.to(decoder.device)[:, -decoder.num_layers:, :])\n",
    "\n",
    "    loss = loss_function(output[:, :-1, :].transpose(1, 2), batch_of_y.to(decoder.device)[:, 1:])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fdci60aaqDUe"
   },
   "outputs": [],
   "source": [
    "def train_epoch(train_generator, model, loss_function, optimizer):\n",
    "    epoch_loss = 0\n",
    "    total = 0\n",
    "    for it, (batch_of_x, batch_of_y) in enumerate(train_generator):\n",
    "        local_loss = train_on_batch(\n",
    "            model, batch_of_x, batch_of_y, optimizer, loss_function)\n",
    "        train_generator.set_postfix({'train batch loss': local_loss})\n",
    "\n",
    "        epoch_loss += local_loss * len(batch_of_x)\n",
    "        total += len(batch_of_x)\n",
    "    \n",
    "    return epoch_loss / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1k43LkCVqDUf"
   },
   "outputs": [],
   "source": [
    "def trainer(count_of_epoch, \n",
    "            batch_size,\n",
    "            model,\n",
    "            dataset,\n",
    "            char2idx,\n",
    "            loss_function,\n",
    "            optimizer,):\n",
    "    iterations = tqdm(range(count_of_epoch))\n",
    "\n",
    "    for it in iterations:\n",
    "        optima = optimizer\n",
    "\n",
    "        number_of_batch = len(dataset) // batch_size + (len(dataset) % batch_size > 0)\n",
    "        generator = tqdm(\n",
    "            batch_generator(dataset, char2idx, batch_size), \n",
    "            leave=False, total=number_of_batch)\n",
    "        \n",
    "        epoch_loss = train_epoch(\n",
    "            train_generator = generator, model = model, \n",
    "            loss_function = loss_function, \n",
    "            optimizer = optima)\n",
    "\n",
    "        iterations.set_postfix({'train epoch loss': epoch_loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4gkPhl-_GBj"
   },
   "source": [
    "### Модель нейросети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "cHykQdETqDUd"
   },
   "outputs": [],
   "source": [
    "class Encoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "        \n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 emb_dim = 10, \n",
    "                 hidden_dim = 10,\n",
    "                 num_layers = 3,\n",
    "                 bidirectional = False):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.num_direction = int(bidirectional + 1)\n",
    "        self.emb_dim = emb_dim # обучаемая матрица эмбедингов\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, emb_dim)\n",
    "\n",
    "        self.encoder = torch.nn.LSTM(\n",
    "            emb_dim, hidden_dim, num_layers, bidirectional=bidirectional)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = self.embedding(input) # shape (batch_size, seq_len, emb_dim)\n",
    "        input = torch.transpose(input, 0, 1) # shape (seq_len, batch_size, emb_dim)\n",
    "        d, (h, c) = self.encoder(input)\n",
    "        return d, torch.transpose(h, 0, 1), torch.transpose(c, 0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9ZlLDdTUqDUd"
   },
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    @property\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def __init__(self,\n",
    "                 vocab_dim,\n",
    "                 output_dim,\n",
    "                 emb_dim = 10, \n",
    "                 hidden_dim = 10,\n",
    "                 num_layers = 3,\n",
    "                 bidirectional = False):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.num_direction = int(bidirectional + 1)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = torch.nn.Embedding(vocab_dim, self.emb_dim)\n",
    "\n",
    "        self.decoder = torch.nn.LSTM(\n",
    "            emb_dim, hidden_dim, num_layers, bidirectional = bidirectional)\n",
    "\n",
    "        self.linear = torch.nn.Linear(\n",
    "            self.num_direction*hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, real=None, h=None, c=None, max_len=50):\n",
    "        batch_size = 1\n",
    "        if h is not None:\n",
    "            batch_size = h.shape[0]\n",
    "        if c is not None:\n",
    "            batch_size = c.shape[0]\n",
    "        if real is not None:\n",
    "            batch_size = real.shape[0]\n",
    "\n",
    "\n",
    "        if real is not None:\n",
    "            input = self.embedding(real)\n",
    "\n",
    "            if h is None:\n",
    "                h = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "            if c is None:\n",
    "                c = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "\n",
    "            input = torch.transpose(input, 0, 1)\n",
    "            h = torch.transpose(h, 0, 1)\n",
    "            c = torch.transpose(c, 0, 1)\n",
    "            d, _ = self.decoder(input, (h, c))\n",
    "            answers = self.linear(d)\n",
    "        else:\n",
    "            input = self.embedding(\n",
    "                torch.tensor(\n",
    "                    [[char2idx['<START>']] for _ in range(\n",
    "                        batch_size)]).long().to(\n",
    "                        self.device\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            if h is None:\n",
    "                h = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "            if c is None:\n",
    "                c = torch.randn(\n",
    "                    (batch_size, self.num_layers, self.num_direction*self.hidden_dim)).to(\n",
    "                        self.device\n",
    "                    )\n",
    "\n",
    "            input = torch.transpose(input, 0, 1)\n",
    "            h = torch.transpose(h, 0, 1)\n",
    "            c = torch.transpose(c, 0, 1)\n",
    "\n",
    "            answers = torch.zeros(\n",
    "                (max_len, input.shape[1], self.output_dim)).to(\n",
    "                    self.device)\n",
    "                \n",
    "            for i in range(max_len):\n",
    "                d, (h, c) = self.decoder(input, (h, c))\n",
    "                answers[i, :, :] = self.linear(d)[0]\n",
    "                input = self.embedding(\n",
    "                    torch.argmax(answers[i:i+1, :, :], dim=-1))\n",
    "\n",
    "        return torch.transpose(answers, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7KY9OTT_GBm"
   },
   "source": [
    "### Выборка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vAM9RmjAqDUf"
   },
   "outputs": [],
   "source": [
    "text = open('input.txt', 'r', encoding='utf-8').read() # should be simple plain text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UtgW1ExSqDUf"
   },
   "outputs": [],
   "source": [
    "char2idx = {'<PAD>':0, '<UNK>': 1, '<START>': 2, '<FINISH>': 3}\n",
    "idx2char = {0: '<PAD>', 1: '<UNK>', 2: '<START>', 3: '<FINISH>'}\n",
    "for item in list(set(text)):\n",
    "    char2idx[item] = len(char2idx)\n",
    "    idx2char[char2idx[item]] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8J_D3vo8qDUf",
    "outputId": "eb391a3e-8791-4404-cc02-e63f27dcf958"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3758"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = [sent.strip() for sent in text.split('\\n') if len(sent.strip()) > 20 and \n",
    "           len(sent.strip()) < 300 ]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPZ5LTRS_GBo"
   },
   "source": [
    "### Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ePn7fVdCqDUf"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_dim=len(char2idx), \n",
    "                  num_layers=2, emb_dim=100, hidden_dim=100)\n",
    "encoder.to(device)\n",
    "decoder = Decoder(vocab_dim=len(char2idx), \n",
    "                  output_dim=len(char2idx), num_layers=2, emb_dim=100, hidden_dim=100)\n",
    "decoder.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()), lr=1e-3)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=char2idx['<PAD>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgZb6LOH_GBp"
   },
   "source": [
    "### Качество модели до обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2A7wODAGq-lQ",
    "outputId": "9f9bb723-2e62-41f1-c16b-00650467050b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ooobPbPЛЛЛ<UNK><UNK><UNK>ppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbP\n",
      "bbbbbbbPbPЛЛЛ<UNK><UNK><UNK>ppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbb\n",
      "pppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPb\n",
      "ажжжжжж111<UNK><UNK><UNK>pppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPb\n",
      "ÇРÇ<UNK><UNK>pppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPppppppp\n",
      "<UNK><UNK><UNK><UNK>жжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжжж\n",
      "2дддppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPppppppp\n",
      "vvppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbb\n",
      "чч<UNK><UNK><UNK>щ<UNK><UNK>pppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppp\n",
      "vvvpppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbbbPbPpppppppbbbb\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    indexes = torch.argmax(\n",
    "        decoder(max_len=100,\n",
    "                h=0.1*torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                        decoder.device\n",
    "                ), \n",
    "                c=torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                    decoder.device\n",
    "                )), dim=-1).detach().cpu().numpy()[0]\n",
    "    list_of_char = []\n",
    "    for idx in indexes:\n",
    "        if idx == char2idx['<FINISH>']:\n",
    "            break\n",
    "        list_of_char.append(idx2char[idx])\n",
    "    print(''.join(list_of_char))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2kFxWjXE_GBq"
   },
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "IIH2giImqDUg"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06a6c6b2222948699447efeb2550bac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer(count_of_epoch=10,\n",
    "        batch_size=64,\n",
    "        model=(encoder, decoder),\n",
    "        dataset=dataset, \n",
    "        char2idx=char2idx,\n",
    "        loss_function=loss_function,\n",
    "        optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhV_ulJ4_GBq"
   },
   "source": [
    "### Качество модели после обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ed8nkY5dqDUg",
    "outputId": "e3ec7055-8e5c-435a-97bc-a6a43335a251"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ы не проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал ва\n",
      "цмлско не проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал в\n",
      "зроно не проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал ва\n",
      "овори не проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал ва\n",
      "яя все подоровал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал\n",
      " не не проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал \n",
      "ько все проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал\n",
      "ко не проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал в\n",
      ". не проворил вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал ва\n",
      "овселько вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал вал\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    indexes = torch.argmax(\n",
    "        decoder(max_len=100,\n",
    "                h=0.1*torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                        decoder.device\n",
    "                ), \n",
    "                c=torch.randn(\n",
    "                    (1, decoder.num_layers, decoder.num_direction*decoder.hidden_dim)).to(\n",
    "                    decoder.device\n",
    "                )), dim=-1).detach().cpu().numpy()[0]\n",
    "    list_of_char = []\n",
    "    for idx in indexes:\n",
    "        if idx == char2idx['<FINISH>']:\n",
    "            break\n",
    "        list_of_char.append(idx2char[idx])\n",
    "    print(''.join(list_of_char))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "rise": {
   "transition": "none"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
